akka {
  stdout-loglevel = "DEBUG"
  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 30s
  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
  }
  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "{{ HOST_IP }}"
      hostname = ${?HOST}
      port = 7661
      port = ${?PORT}
    }
  }
  cluster {
    seed-nodes = []
    auto-down = on
    auto-down-unreachable-after = 10s
    min-nr-of-members = 1
  }
}

couchbase {
  connection-string = "{{ CB_HOST_URL }}"
  buckets {
    main {  # Bucket for CouchbaseNodeDiscoverer.
      name = "{{ CB_BUCKET_ARCHIVE_MAIN_NAME }}"
      password = "{{ CB_BUCKET_ARCHIVE_MAIN_PASS }}"
      key-prefix = "{{ CB_BUCKET_ARCHIVE_MAIN_KEY_PREFIX }}"
    }
    offset {  # Bucket for kafka topic partition offset.
      name = "{{ CB_BUCKET_ARCHIVE_OFFSET_NAME }}"
      password = "{{ CB_BUCKET_ARCHIVE_OFFSET_PASS }}"
      key-prefix = "{{ CB_BUCKET_ARCHIVE_OFFSET_KEY_PREFIX }}"
    }
  }
}

kafka {
    consumer {
      topic = "{{ KAFKA_TOPIC_ARCHIVE_CONSUMER }}"
      brokers = "{{ KAFKA_BROKERS_ARCHIVE_CONSUMER }}"
    }
  }

aws {
  region-name = "{{ AWS_REGION_NAME }}"
  s3 {
    bucket-name = "{{ AWS_S3_BUCKET_NAME }}"
    compression-type = "{{ AWS_S3_COMPRESSION_TYPE }}" // GZIP, BZIP2, LZOP, others for non-compression
    need-encrypt = {{ AWS_S3_NEED_ENCRYPT }} // true or false
  }
}

ssp-archive {
  cluster-group = "ssp-archive"
  # Time to wait before considering cluster to be fully formed (i. e. time during which we expect all nodes to start)
  cluster-rampup-time = 30s
  consume-batch-size = {{ BATCH_SIZE }}

  retries {
    send {
      max = 10
      interval = 500ms
    }
    couchbase {
      max = 60 #times
      interval = 2000ms
    }
  }

}
